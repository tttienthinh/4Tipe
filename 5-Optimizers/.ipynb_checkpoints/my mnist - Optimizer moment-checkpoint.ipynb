{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons essayer de reconnaitre des nombres écrit à la main grace à un réseau de neuronne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "y_train = np.zeros((len(Y_train), 10))\n",
    "y_train[np.arange(len(Y_train)), Y_train] = 1 # to categorical\n",
    "y_test = np.zeros((len(Y_test), 10))\n",
    "y_test[np.arange(len(Y_test)), Y_test] = 1 # to categorical \n",
    "# cela permet de transformer la sortie en une liste [0, 0, 0, 0, 0, 0, 0, 0 ,0, 0, 0] \n",
    "# avec un 1 à l'indice n\n",
    "# par exemple si le nombre cherché est 2 : [0, 0, 1, 0, 0, 0, 0, 0 ,0, 0, 0] \n",
    "\n",
    "x_train = X_train.reshape(-1, 28*28)/255 # 28*28 = 784\n",
    "x_test = X_test.reshape(-1, 28*28)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fonction activation Sigmoid\n",
    "\"\"\"\n",
    "def sigmoid(x, derive=False):\n",
    "    \"\"\"\n",
    "    Fonction Sigmoid\n",
    "    \"\"\"\n",
    "    if derive:\n",
    "        return np.exp(-x) / ((1+np.exp(-x)) ** 2)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\"\"\"\n",
    "Fonction activation Softmax\n",
    "https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n",
    "\"\"\"\n",
    "def softmax(y, derivative=False):\n",
    "    result = []\n",
    "    for x in y:\n",
    "        exps = np.exp(x - x.max()) # permet d'éviter une exponentielle trop grande\n",
    "        if derivative:\n",
    "            result.append(exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0)))\n",
    "        else:\n",
    "            result.append(exps / np.sum(exps, axis=0))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_n=2, output_n=2, lr=0.1, activation=None):\n",
    "        \"\"\"\n",
    "        Crée un layer de n neuronne connecté aux layer de input neuronnes\n",
    "        \"\"\"\n",
    "        # input_n le nombre d'entrée du neuronne\n",
    "        # output_n le nombre de neuronne de sortie\n",
    "        self.weight = np.random.randn(input_n, output_n)\n",
    "        self.input_n = input_n\n",
    "        self.output_n = output_n\n",
    "        self.lr = lr # learning rate\n",
    "\n",
    "        # the name of the layer is 1\n",
    "        # next one is 2 and previous 0\n",
    "        self.predicted_output_ = 0\n",
    "        self.predicted_output  = 0\n",
    "        self.input_data = 0\n",
    "\n",
    "        # Fonction d'activation\n",
    "        self.activation = activation if activation != None else lineaire\n",
    "\n",
    "    def calculate(self, input_data):\n",
    "        \"\"\"\n",
    "        Calcule la sortie\n",
    "        \"\"\"\n",
    "        self.input_data = input_data\n",
    "        # self.input_data = np.concatenate((input_data, np.ones((len(input_data), 1))), axis=1)\n",
    "        y1 = np.dot(self.input_data, self.weight)\n",
    "        z1 = self.activation(y1)\n",
    "        self.predicted_output_ = y1\n",
    "        self.predicted_output = z1\n",
    "        return y1, z1\n",
    "\n",
    "    def learn(self, e_2):\n",
    "        \"\"\"\n",
    "        Permet de mettre à jour les weigths\n",
    "        \"\"\"\n",
    "        e1 = e_2 / self.output_n * self.activation(self.predicted_output_, True)\n",
    "        # e_0 is for the next layer\n",
    "        # e_0 = np.dot(e1, self.weight.T)\n",
    "        e_0 = np.dot(e1, self.weight.T)\n",
    "        dw1 = np.dot(e1.T, self.input_data)\n",
    "        self.weight -= dw1.T * self.lr\n",
    "        return e_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mean Square Error function\n",
    "Je l'utilise mais il serait mieux d'utiliser cross entropy normalement\n",
    "\"\"\"\n",
    "def mse(predicted_output, target_output, derivate=False):\n",
    "    if derivate:\n",
    "        return (predicted_output - target_output) *2 \n",
    "    return ((predicted_output - target_output) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, layers=[], loss_function=None):\n",
    "        self.layers = layers\n",
    "        self.loss = []\n",
    "        self.lr = 0.1\n",
    "        self.loss_function = loss_function  \n",
    "\n",
    "    def predict(self, input_data):\n",
    "        predicted_output = input_data  # y_ is predicted data\n",
    "        for layer in self.layers:\n",
    "            predicted_output_, predicted_output = layer.calculate(predicted_output) # output\n",
    "        return predicted_output\n",
    "\n",
    "    def predict_loss(self, input_data, target_output):  # target_output is expected data\n",
    "        predicted_output = self.predict(input_data)  # y_ is predicted data\n",
    "        loss = self.loss_function(predicted_output, target_output)\n",
    "        return predicted_output, loss\n",
    "    \n",
    "    def compute_accuracy(self, x_val, y_val):\n",
    "        predictions = []\n",
    "        for x, y in zip(x_val, y_val):\n",
    "            output = self.predict([x])\n",
    "            pred = np.argmax(output[0])\n",
    "            predictions.append(pred == np.argmax(y))\n",
    "        return np.mean(predictions)\n",
    "\n",
    "    def backpropagation(self, input_data, target_output, batch=None):\n",
    "        n = len(input_data)\n",
    "        if batch is None:\n",
    "            batch = n\n",
    "        step = n//batch\n",
    "        losses = []\n",
    "        for i in range(step):\n",
    "            b_input_data = input_data[::step]\n",
    "            b_target_output = target_output[::step]\n",
    "            predicted_output, loss = self.predict_loss(b_input_data, b_target_output)\n",
    "            d_loss = self.loss_function(predicted_output, b_target_output, True) # dérivé de loss dy_/dy\n",
    "            # Entrainement des layers\n",
    "            for i in range(len(self.layers)):\n",
    "                d_loss = self.layers[-i - 1].learn(d_loss)\n",
    "            losses.append(loss)\n",
    "        loss = sum(losses)/len(losses)\n",
    "        self.loss.append(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le model est de taille 784 -> 32 sigmoid -> 16 sigmoid -> 10 softmax\n",
    "np.random.seed(2) # permet de rendre le programme reproductible\n",
    "model = Model([\n",
    "    Layer(784, 32, 0.001, sigmoid),\n",
    "    Layer(32, 16, 0.001, sigmoid),\n",
    "    Layer(16, 10, 0.001, softmax),\n",
    "], mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 loss : 0.151419233487184, acc : 11.51 %\n",
      "Epoch : 1 loss : 0.1157290798817189, acc : 12.01 %\n",
      "Epoch : 2 loss : 0.1081752170670607, acc : 10.0 %\n",
      "Epoch : 3 loss : 0.10180303771541105, acc : 10.32 %\n",
      "Epoch : 4 loss : 0.09722655973188295, acc : 13.24 %\n",
      "Epoch : 5 loss : 0.09452930048303006, acc : 13.27 %\n",
      "Epoch : 6 loss : 0.09420079021127374, acc : 13.34 %\n",
      "Epoch : 7 loss : 0.09388583492810146, acc : 13.67 %\n",
      "Epoch : 8 loss : 0.09358154596426761, acc : 14.03 %\n",
      "Epoch : 9 loss : 0.09328117087748627, acc : 14.24 %\n",
      "Epoch : 10 loss : 0.09298778701802908, acc : 14.84 %\n",
      "Epoch : 11 loss : 0.0926988624362722, acc : 15.11 %\n",
      "Epoch : 12 loss : 0.0924154419825512, acc : 15.55 %\n",
      "Epoch : 13 loss : 0.09213662672953978, acc : 15.87 %\n",
      "Epoch : 14 loss : 0.09186268533033595, acc : 16.3 %\n",
      "Epoch : 15 loss : 0.09159327645562816, acc : 16.76 %\n",
      "Epoch : 16 loss : 0.09132842925791806, acc : 17.22 %\n",
      "Epoch : 17 loss : 0.09106799753211056, acc : 17.92 %\n",
      "Epoch : 18 loss : 0.09081196154177407, acc : 18.27 %\n",
      "Epoch : 19 loss : 0.09056025572738133, acc : 18.81 %\n",
      "Epoch : 20 loss : 0.09031286496690681, acc : 19.13 %\n",
      "Epoch : 21 loss : 0.0900697647235585, acc : 19.64 %\n",
      "Epoch : 22 loss : 0.08983095376259054, acc : 20.1 %\n",
      "Epoch : 23 loss : 0.08959642973161809, acc : 20.55 %\n",
      "Epoch : 24 loss : 0.0893662008850668, acc : 20.89 %\n",
      "Epoch : 25 loss : 0.08914027404235514, acc : 21.29 %\n",
      "Epoch : 26 loss : 0.08891865882675982, acc : 21.69 %\n",
      "Epoch : 27 loss : 0.08870136075005296, acc : 22.01 %\n",
      "Epoch : 28 loss : 0.08848838273501398, acc : 22.24 %\n",
      "Epoch : 29 loss : 0.08827972085815047, acc : 22.5 %\n",
      "Epoch : 30 loss : 0.0880753650394452, acc : 22.72 %\n",
      "Epoch : 31 loss : 0.08787529647848584, acc : 23.06 %\n",
      "Epoch : 32 loss : 0.08767948828333874, acc : 23.27 %\n",
      "Epoch : 33 loss : 0.0874879041105026, acc : 23.54 %\n",
      "Epoch : 34 loss : 0.08730049900195966, acc : 23.72 %\n",
      "Epoch : 35 loss : 0.08711721888714853, acc : 24.1 %\n",
      "Epoch : 36 loss : 0.08693800165246961, acc : 24.33 %\n",
      "Epoch : 37 loss : 0.08676277722056591, acc : 24.55 %\n",
      "Epoch : 38 loss : 0.08659146876791701, acc : 24.81 %\n",
      "Epoch : 39 loss : 0.08642399314183263, acc : 25.07 %\n",
      "Epoch : 40 loss : 0.08626026211216349, acc : 25.3 %\n",
      "Epoch : 41 loss : 0.08610018293825808, acc : 25.49 %\n",
      "Epoch : 42 loss : 0.08594365955414401, acc : 25.78 %\n",
      "Epoch : 43 loss : 0.0857905931531849, acc : 26.09 %\n",
      "Epoch : 44 loss : 0.08564088323736645, acc : 26.24 %\n",
      "Epoch : 45 loss : 0.08549442813863024, acc : 26.35 %\n",
      "Epoch : 46 loss : 0.08535112589631406, acc : 26.65 %\n",
      "Epoch : 47 loss : 0.085210874675363, acc : 26.84 %\n",
      "Epoch : 48 loss : 0.08507357346336095, acc : 27.01 %\n",
      "Epoch : 49 loss : 0.08493912237497195, acc : 27.11 %\n"
     ]
    }
   ],
   "source": [
    "# Entrainement\n",
    "for i in range(50):\n",
    "    loss = model.backpropagation(x_train, y_train)\n",
    "    acc = model.compute_accuracy(x_test, y_test)\n",
    "    print(f\"Epoch : {i} loss : {loss}, acc : {round(acc*100, 2)} %\")\n",
    "# Sur un des test précédement réalisé, après 3000 entrainement, on obtient 70% d'accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sauvegarde du model\n",
    "pickle.dump( model, open( \"demo.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test sur le model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load( open( \"demo.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 0 accuracy is 28.06\n",
      "For 1 accuracy is 80.0\n",
      "For 2 accuracy is 6.49\n",
      "For 3 accuracy is 7.52\n",
      "For 4 accuracy is 14.05\n",
      "For 5 accuracy is 6.95\n",
      "For 6 accuracy is 47.08\n",
      "For 7 accuracy is 39.2\n",
      "For 8 accuracy is 9.34\n",
      "For 9 accuracy is 23.79\n"
     ]
    }
   ],
   "source": [
    "# L'accuracy n'est pas le meme selon les nombres\n",
    "for i in range(10):\n",
    "    indexer = (Y_test == i)\n",
    "    acc = model.compute_accuracy(x_test[indexer], y_test[indexer])\n",
    "    print(f\"For {i} accuracy is {round(acc * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[2], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([x_test[2]]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11776384, 0.03745747, 0.13988358, 0.17766919, 0.13496387,\n",
       "        0.01081449, 0.28744254, 0.04125383, 0.02985277, 0.02289842]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([x_test[1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4Tipe",
   "language": "python",
   "name": "4tipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
