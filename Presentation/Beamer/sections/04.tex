
\section{IV - Problème de reproduction de l'opérateur XOR}
\begin{frame}{IV - Problème de reproduction de l'opérateur XOR}
\begin{block}{Problème non linéairement séparables}
Un perceptron ou une couche de perceptron est incapable de reproduire des opérateurs non linéairement séparables. \\
Il faut alors mettre les perceptrons en série, pour former des couches cachées, pour pouvoir reproduire ces opérateurs. \\
\end{block}
\begin{figure}
	\centering
    \includegraphics[height=100px]{1-Reseau.png}
	\caption{Schéma d'un réseau de neurones}
\end{figure}
\end{frame}

\begin{frame}{IV - Problème de reproduction de l'opérateur XOR}
\begin{block}{Le XOR nécessite un réseau}
Le XOR, ou exclusif, est un opérateur non linéairement séparable. \\
On peut par exemple démontrer que l'ajout d'une couche cachée de 2 perceptrons suffit à reproduire l'opérateur XOR. \\
\begin{figure}
	\centering
    \includegraphics[width=100px]{2-XOR.jpg}
	\caption{Schéma de l'opérateur XOR}
\end{figure}
\end{block}	
\end{frame}

\begin{frame}{IV - Problème de reproduction de l'opérateur XOR}
\begin{figure}
	\centering
    \includegraphics[width=230px]{3-Model.png}
	\caption{Schéma du réseau de neurone reproduisant le XOR}
\end{figure}
\begin{block}{Descente de gradient}
$w \leftarrow w - t \dfrac{\partial f}{\partial w}$ où $t$ est le taux d'apprentissage et $f$ la fonction de coût
\end{block}
\begin{exampleblock}{Exemple}
• $\dfrac{\partial f}{\partial w^2_1} = 2(s - s_{attendue})\sigma_2 'y^1_1$ \\
• $\dfrac{\partial f}{\partial w^1_{1\to 1}} = 2(s - s_{attendue})\sigma_2 'w^2_1 \sigma _{1} ' e_{1}$
\end{exampleblock}

\end{frame}

\begin{frame}
\begin{figure}
	\centering
    \includegraphics[width=300px]{4-XOR.png}
	\caption{Courbe de décroissance de l'erreur}
\end{figure}
\end{frame}

\begin{frame}{IV - Résultat obtenu}
\begin{block}{Données}
• 4 données \\
• 300 générations \\
• Erreur minimale atteinte 0.036
\end{block}
\begin{align*} 
Entr\acute{e} e\, :
\begin{pmatrix}
0 & 0 \\ 
0 & 1 \\ 
1 & 0 \\ 
1 & 1 
\end{pmatrix} 
		&\to  
\mathlarger{\mathlarger{\sigma_{couche1}}}
\left( \centerdot \times
\begin{pmatrix}
0.85 & 5.42 \\ 
0.85 & 5.40 \\ 
0.14 & 0.44
\end{pmatrix}
\right) \\ 
 		&\to
\mathlarger{\mathlarger{\sigma_{couche2}}}
\left( \centerdot \times
\begin{pmatrix}
-18.39\\ 
14.42 \\ 
0.02
\end{pmatrix}
\right) \\
 		&\to
Sortie\, :
\begin{pmatrix}
0.12 \\ 
0.81 \\ 
0.81 \\ 
0.24
\end{pmatrix}
Sortie_{attendue}\, :
\begin{pmatrix}
0 \\ 
1 \\ 
1 \\ 
0
\end{pmatrix}
\end{align*}
\end{frame}